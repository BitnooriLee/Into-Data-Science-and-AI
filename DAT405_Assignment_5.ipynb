{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BitnooriLee/Into-Data-Science-and-AI/blob/main/Assignment5_2022_Bitnoori_Lee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRy420r1FdRD"
      },
      "source": [
        "$\\qquad$ $\\qquad$$\\qquad$  **DAT405 Introduction to Data Science and AI, LP4 2022** <br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$                   **Assignment 5: Reinforcement learning** <br />\n",
        "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Bitnoori Lee(18hours), Personal No. 19881205-3323, Email bitnoori.lee@gmail.com** <br />\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "General guidelines:\n",
        "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
        "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
        "*   Your name, personal number and email address should be specified above.\n",
        "*   All tables and other additional information should be included in this notebook.\n",
        "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJHCYa_vFdRJ"
      },
      "source": [
        "Self-check \n",
        "1. Have you answered all questions to the best of your ability? \n",
        "2. Anything else you can easily check? (details, terminology, arguments, commenting for code etc.?) \n",
        "\n",
        "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
        "*\tPresent clear arguments\n",
        "*\tPresent the results in a pedagogical way\n",
        "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
        "*\tGive correct solutions\n",
        "*\tMake sure that the code is well commented \n",
        "\n",
        "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (RLAssignment.ipynb) should be submitted with answers and code written in it. Ipython notebook is mandatory submission. And also submit HTML version of it for easy readibility (goto File/Download as). NO OTHER FORMAT SHALL BE ACCEPTED.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf2Shw2pFdRK"
      },
      "source": [
        "# Primer\n",
        "\n",
        "## Decision Making\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
        "two parts. First, how do we learn about the world? This involves both the\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
        "currently know about the world, how should we decide what to do, taking into\n",
        "account future events and observations that may change our conclusions?\n",
        "Typically, this will involve creating long-term plans covering possible future\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
        "known to produce good results and experiment with something new is known\n",
        "as the **exploration-exploitation dilemma**.\n",
        "\n",
        "## The exploration-exploitation trade-off\n",
        "\n",
        "Consider the problem of selecting a restaurant to go to during a vacation.Lets say the\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\n",
        "website suggests that **King‚Äôs Arm** is really good! It is tempting to try it out. But\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
        "in which case you will regret going there. On the other hand, it could also be\n",
        "much better. What should you do?\n",
        "It all depends on how much information you have about either restaurant,\n",
        "and how many more days you‚Äôll stay in town. If this is your last day, then it‚Äôs\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King‚Äôs\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\n",
        "trying out **King‚Äôs Arm** is a good bet. If you are lucky, you will be getting much\n",
        "better food for the remaining time, while otherwise you will have missed only\n",
        "one good meal out of many, making the potential risk quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Gkc3DwFdRM"
      },
      "source": [
        "## Overview\n",
        "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We shall first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
        "\n",
        "\n",
        "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C31k2F8fFdRN"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YywTHbydFdRO"
      },
      "source": [
        "Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making. It is a discrete time (distinct points in time) stochastic (randomly determined) process.\n",
        "\n",
        "MDPs are made up of 4 parts:  \n",
        "S: Finite set of states (Ex: s<sub>1</sub>, s<sub>2</sub> ... s<sub>N</sub>)  \n",
        "A: Finite set of actions (Ex: North, South, East, West)  \n",
        "P<sub>a</sub>(s,s'): Probability that action *a* in state *s* at time *t* will lead to state *s'* at time *t + 1*  \n",
        "R<sub>a</sub>(s,s'): Immediate reward received after moving from state *s* to state *s'* by action *a*\n",
        "\n",
        "An agent acts in an MDP at time *t*, by taking certain action *a* in state *s*, going to state *s'*, and getting a reward *r* from the world. It then repeats the process for certain no. of times, either finite or infinite.\n",
        "\n",
        "We also include a $5^{th}$ part in the description of an MDP called Gamma $\\gamma$.  \n",
        "$\\gamma$: The discount factor between 0 (inclusive) and 1 (exclusive). This determines how much credit you want to give to the future. If you think that the future reward is as important as the current reward you would set this to 0.99999. If you don't care about the future rewards you would set this to 0 and you only care about the current reward. For example, if your discount factor is 0.8 and after 5 steps you get a reward of 4 the present value of that reward is $0.8^4 * 5$ or ~2.\n",
        "\n",
        "An MDP is a collection of states such that each state has a selection of actions associated with them. With each state-action pair comes a reward *r* (can be 0). Define a policy function: $\\pi: s \\rightarrow a$, which tells which action to take at each state.\n",
        "  \n",
        "We now use the famous dynamic programming equation, also known as Bellman Equation, to define optimality in an MDP. The following equation defines what we call the **value function** of state *s* following some fixed policy $\\pi$:  \n",
        "\n",
        "$$V^\\pi(s) = \\sum_{s'} P_{\\pi(s)}(s,s') [R_{\\pi(s)}(s,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "We call $V^\\pi$ as the value of policy $\\pi$.  \n",
        "  \n",
        "Now, to find the **optimal** policy you will need to find the action that gives the highest reward.  \n",
        "\n",
        "$$V^*(s) = max_a \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
        "\n",
        "A major drawback of MDPs is called the \"Curse of Dimensionality\". This states that the more states/actions you have the more computational difficult it is to solve.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rleYs4DgFdRP"
      },
      "source": [
        "## Question 1 (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skXe5kaCFdRQ"
      },
      "source": [
        "For the first question of the notebook, we give a quick example of an MDP. We would to see if you can put the definitions above into practice.\n",
        "\n",
        "**Question a**: Given the following deterministic MDP (you select North, you move North), what is the optimal policy (path with the most points)?\n",
        "  \n",
        "*Notes*:  \n",
        "  * The number in the box is the reward.  \n",
        "  * Once you hit the end you are done. (Absorbing state)\n",
        "  * S is the starting point.  \n",
        "  * F is the ending point.  \n",
        "  * Use N for North, E for East, S for South, and W for West. Not all actions are available at each state, for example, you can't choose N and W at starting state, as there exists no valid next states in those directions.  \n",
        "  * Pass the directions as a single string. Ex: ESWN will make a circle.  \n",
        "  \n",
        "\n",
        "\n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|S|1|1|\n",
        "|1 |0|1|  \n",
        "|-1|-1|0|  \n",
        "|0 |0|F|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11gSLTOyx9Gc"
      },
      "source": [
        "**Answer a**: \n",
        "\n",
        "We can find the optimal policy by hitting the states where you get the point and avoiding states that reduce the point.\n",
        "\n",
        "The optimal policy is \n",
        "In state, \n",
        "\n",
        "(0,0): a = E or S \n",
        "\n",
        "(1,0): a = E\n",
        "\n",
        "(2,0): a = S or W\n",
        "\n",
        "(0,1): a = E\n",
        "\n",
        "(1,1): a = N, E or W\n",
        "\n",
        "(2,1): a = N\n",
        "\n",
        "(0,2): a = N\n",
        "\n",
        "(1,2): a = N, E or S\n",
        "\n",
        "(2,2): a = N\n",
        "\n",
        "(0,3): a = E\n",
        "\n",
        "(1,3): a = W\n",
        "\n",
        "(2,3): a = absorbed(finished)\n",
        "\n",
        "where the optimal path is (0.0) ->(0,1) -> (1,1) -> (1,0) -> (2,0) -> (2,1)-> (2,2)-> (2,3) SENESSS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClfbUCiLFdRR"
      },
      "source": [
        "Question b,c will attempt to firm up your knowledge of the parts of an MDP. Just remember that for a state denoted by (x,y), state N/E/S/W to that are (x,y-1),(x+1,y),(x,y+1),(x-1,y) respectively. We take (0,0) as the starting state S.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2VVHaHpw-em"
      },
      "source": [
        "\n",
        "**Question b:** What is the probability of going from state (1,0) to state (2,0) using action E ? ( i.e,  $P_E((1,0),(2,0))$ )\n",
        "\n",
        "**Answer b:** In deterministic MDP, the agent chooses a unique action in each state. At state (1,0), action E chosen to (2,0), so the probability is 1. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6AmLdY1w-Lp"
      },
      "source": [
        "**Question c:** What is the reward for moving from state (1,0) to state (2,0) ? ( i.e, $R_E((1,0),(2,0))$ )\n",
        "\n",
        "**Answer c:**\n",
        "The reward for the action E \n",
        "(1,0): a = E, the reward: the value collect once it landed in state (2,0) is 1.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKd-R3qFdRS"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ26WF4VFdRT"
      },
      "source": [
        "\n",
        "The value function is based on the Bellman Equation for optimal value, which we recall here:  \n",
        "$$V^*(s) = max_a \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "The value iteration (VI) is one algorithm that can be used to find the optimal policy ($\\pi^*$). Note that for any policy $\\pi^*$ to be optimal, it must satisfy the Bellman equation for optimal value function $V^*$. For any candidate $V^*$, it must be such that plugging it in the RHS (right-hand-side) of Bellman equation should give the same $V^*$ again (by the recursive nature of this equation). This property forms the basis of VI algorithm. Essentially, due to certain mathematical results, repeated application of RHS to any intial value function $V^0(s)$ will eventually lead to the value $V^*$ which statifies the Bellman equation. Once converged*, one can extract the optimal actions by simply noting the actions that satisfy the equation.\n",
        "\n",
        "\n",
        "Note: By 'converge', we mean that the quantity of interest doesn't change anymore by further iterations of the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJnmoVybFdRT"
      },
      "source": [
        "The value iteration algorithm practically procedes as follows:\n",
        "\n",
        "```\n",
        "epsilon is a small value, threshold\n",
        "V_k[s] = 0 for all s\n",
        "while |V_k[s]-V_k-1[s]| > epsilon for any s\n",
        "do\n",
        "    for each state s\n",
        "    do\n",
        "        V_k[s] = max_a Œ£_s' P_a(s,s')*(R_a(s,s‚Ä≤) + Œ≥*V_k‚àí1[s‚Ä≤])\n",
        "    end\n",
        "end\n",
        "\n",
        "for each state s\n",
        "do\n",
        "    œÄ(s)=argmax_a ‚àë_s‚Ä≤ P_a(s,s')*(R_a(s,s‚Ä≤) + Œ≥*V_k[s‚Ä≤])\n",
        "end\n",
        "\n",
        "return œÄ, V_k\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGYG3DiYFdRU"
      },
      "source": [
        "Example: Below is a 3x3 grid. We are going to walk through a few iterations to firm up your understanding. Lets assume this time that success of taking any action is 0.8. Meaning if we take E from a valid state (x,y), we will go (x+1,y) 0.8 percent of time, but remain in same state the remaining time. We will have a discount factor ($\\gamma$) of 0.9. Assume $V^0(s')=0$ for all s'. \n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|0|0|\n",
        "|0|10|0|  \n",
        "|0|0|0|  \n",
        "\n",
        "\n",
        "**Iteration 1**: It is trivial, V(s) becomes the $max_a \\sum_{s'} P_a(s,s') R_a(s,s')$ since $V^0$ was zero for s'.\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|8|0|\n",
        "|8|2|8|  \n",
        "|0|8|0|  \n",
        "  \n",
        "**Iteration 2**:  \n",
        "  \n",
        "Staring with cell (0,0): We find the expected value of each move:  \n",
        "Action N: 0  \n",
        "Action E: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action S: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action W: 0\n",
        "\n",
        "Hence any action between E and S would have been best at this stage.\n",
        "\n",
        "Similarly for cell (1,0):\n",
        "\n",
        "Action S: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action S is the maximizing action)  \n",
        "\n",
        "Similar calculations for remaining cells give us:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|5.76|10.88|5.76|\n",
        "|10.88|8.12|10.88|  \n",
        "|5.76|10.88|5.76|  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY3YJ4E3FdRU"
      },
      "source": [
        "## Question 2 (2 points)\n",
        "Please code the value iteration algorithm just described, and show the optimal value function of the above 3x3 grid problem at convergence (the value function doesn't change anymore)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "x_vma8ilAKXK",
        "outputId": "f28a90e0-bd0b-4467-9067-984432d060d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State:\n",
            "[[ 0  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0  0]]\n",
            "Converged State:\n",
            "[[44.7917635  51.12689178 44.7917635 ]\n",
            " [51.12689178 47.23078789 51.12689178]\n",
            " [44.7917635  51.12689178 44.7917635 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprint('Policy:')\\nprint(Pi) \\n#Reverse the order of elements along axis 0 \\nprint('Policy_Ordered:')\\nprint(np.flipud(Pi))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "#functoin array of possible moves for 3x3 grid\n",
        "def getPossibleAction(state, x, y):\n",
        "\n",
        "    #array of possible actions \n",
        "    possibleAction = []\n",
        "\n",
        "    if(x > 0): \n",
        "        possibleAction.append(('W', x-1,y)) \n",
        "    \n",
        "    if(x < state[0]-1): \n",
        "        possibleAction.append(('E', x+1,y))\n",
        "\n",
        "    if(y > 0): \n",
        "        possibleAction.append(('S',x,y-1))\n",
        "\n",
        "    if(y < state[1]-1): \n",
        "        possibleAction.append(('N', x,y+1))\n",
        "         \n",
        "    return possibleAction\n",
        "\n",
        "\n",
        "def getRewards(x, y, lastV, reward, successAction = 0.8, gamma = 1): \n",
        "    V = -10000\n",
        "    Pi = ''\n",
        "    p = getPossibleAction(reward.shape,x,y)\n",
        "\n",
        "    for state1,x1,y1 in p:\n",
        "        # calculate reward (stay, move)\n",
        "        nextV = successAction * (reward[y1,x1]+gamma*lastV[y1,x1]) + (1-successAction)*(reward[y,x]+gamma*lastV[y,x])\n",
        "\n",
        "        #Replace reward and save policy if the next one is bigger than before\n",
        "        if(nextV > V):\n",
        "            V = nextV\n",
        "            Pi = state1\n",
        "    return Pi, V\n",
        "\n",
        "\n",
        "#Function to find the optimal policy by iterating all state, calling getRewards\n",
        "def valueIteration(lastV, reward, epsilon=0.1, gamma=0.9):\n",
        "    \n",
        "    (row,col) = reward.shape\n",
        "    V = np.zeros((row, col))\n",
        "    Pi = np.zeros((row, col), dtype=str)\n",
        "    #iteration loop\n",
        "    while True:\n",
        "        for i in range(row):\n",
        "            for j in range(col):\n",
        "                Pi[i,j], V[i,j] = getRewards(j, i, lastV, reward, gamma = 0.9)\n",
        "        \n",
        "        if (np.abs(V-lastV)<epsilon).all():\n",
        "            return Pi, V\n",
        "        lastV = V.copy()\n",
        "\n",
        "#Reward matrix\n",
        "rewards = np.array([(0,0,0),(0,10,0),(0,0,0)])\n",
        "#Value matrix \n",
        "initV = 0*np.ones(rewards.shape)\n",
        "# call function with initialized parameters and epsilon=0.1\n",
        "Pi, V = valueIteration(initV, rewards, epsilon=0.1)\n",
        "\n",
        "\n",
        "# The arrays need to be flipped to show the grid correctly:\n",
        "print('Initial State:')\n",
        "print(rewards)\n",
        "print('Converged State:')\n",
        "print(V)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUhSpOJJFdRV"
      },
      "source": [
        "## Reinforcement Learning (RL)\n",
        "Until now, we understood that knowing the MDP, specifically $P_a(s,s')$ and $R_a(s,s')$ allows us to efficiently find the optimal policy using value iteration algorithm, but RL or decision making under uncertainity arises from the question of making optimal decisions without knowing the true world model (MDP in this case).\n",
        "\n",
        "So far we have defined the value of a state $V^\\pi$ (for fixed policy). Let us now define the value of an action, $Q^\\pi$:\n",
        "\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "i.e, the value of taking action *a* from state *s* and then following $\\pi$ onwards. Similarly, the optimal Q-value equation is:\n",
        "\n",
        "$$Q^*(s,a) = \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Q-learning algorithm can be used by an agent unaware of its surroundings (unknown MDP). All it can do is take an action *a* at time *t* from state *s* and observe the reward *r* and next state *s'*, and repeat this process again. So how it can learn to act optimally under such uninformative conditions ? Answer is using Q-learning. Without going into its justification, we simply state the main-update rule of this algorithm below:\n",
        "\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/5anbos4s9luoayb32jk6w3wy3w4jk3g3.png)\n",
        "\n",
        "Where we simply maintain Q(s,a) value for each state-action pair in a table. It is proven to converge to the optimal policy of the underlying unknown MDP for a small learning rate $\\alpha$ (you have to play around with values ranging from 0.1 to 0.00001)\n",
        "## OpenAI Gym ($\\leq 0.19.0$)\n",
        "\n",
        "We shall use already available simulators for different environments (world) using the popular OpenAI Gym library. It just implements [differnt types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as [Chain enviroment](https://gym.openai.com/envs/NChain-v0/).\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
        "\n",
        "## Question 3 (1 point)\n",
        "Basically, there are 5 states, and two actions 'a' and 'b'. Each transition (s,a,s') is noted with its corresponding reward. You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the \"$\\epsilon$-greedy Q-learning\" algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Take $\\gamma=0.95$. \n",
        "\n",
        "Hints:\n",
        "* You can refer to the Q-learning Jupyter notebook shown in class, uploaded on Canvas.\n",
        "* Try some decreasing sequences of $\\epsilon$ (1/t etc.) with small fixed learning rate $\\alpha$.\n",
        "* Latest Gym version (0.23.1) does not have the Chain environment, you need to install any version $\\leq 0.19.0$. You can do so in Google colab by command: \" !pip install gym==version \".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyMImv9HC0VG",
        "outputId": "5dbce73a-a2d6-40f5-db62-06943fdca9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.19.0 in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.19.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.19.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#change version \n",
        "!pip install gym==0.19.0\n",
        "#!pip install gym-toytext\n",
        "\n",
        "\n",
        "#import gym_toytext\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "# Define environment\n",
        "env = gym.make(\"NChain-v0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz2CpBmWDctd"
      },
      "outputs": [],
      "source": [
        "#Define Q_learn function \n",
        "\n",
        "def Q_learn(num_episodes, gamma, learning_rate, epsilon): \n",
        "\n",
        "# initialize the Q table 5 states and 2 actions \n",
        " Q = np.zeros([5, 2])\n",
        " episode_list = [5000,10000,15000,20000]\n",
        "#Training Q table, code taken from q_learning_intro.ipynb\n",
        " for episode in range(num_episodes):\n",
        "\t  state = env.reset()\n",
        "\t  done = False\n",
        "\t  while done == False:\n",
        "        # First we select an action:\n",
        "\t\t  if random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
        "\t\t\t  action = env.action_space.sample() # Explore action space\n",
        "\t\t  else:\n",
        "\t\t\t  action = np.argmax(Q[state,:]) # Exploit learned values\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "\t\t  new_state, reward, done, info = env.step(action)\n",
        "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "\t\t  prediction_error = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
        "\t\t  Q[state,action] += learning_rate*prediction_error \n",
        "\t\t  state = new_state\n",
        "\n",
        "\t # if episode == 5000 or episode == 10000 or episode == 15000 or 20000:\n",
        "\t #\t print(Q)\n",
        "\t\t\n",
        "\t  if episode < 5 or episode > num_episodes - 5: # print Q the first and last few iteration \n",
        "\t    \tprint(\"Episode:\", episode)\n",
        "\t    \tprint(Q)\n",
        "\t    \tprint()\n",
        " #return Q\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMDQ8arWLCXo",
        "outputId": "ea55fe69-4da9-4fe8-f248-890d7ae44eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epsilon: 0.1\n",
            "Episode: 0\n",
            "[[ 9.55716654 20.81333523]\n",
            " [18.95652706  6.25674066]\n",
            " [20.38866776  5.02230233]\n",
            " [ 6.06766717 21.25775231]\n",
            " [44.96330961 13.33475044]]\n",
            "\n",
            "Episode: 1\n",
            "[[25.06042635 30.25299187]\n",
            " [30.0605527  20.42468341]\n",
            " [32.4773424  14.63473144]\n",
            " [11.44434261 33.93588155]\n",
            " [64.77055885 28.84845855]]\n",
            "\n",
            "Episode: 2\n",
            "[[31.2700102  33.97476989]\n",
            " [34.21859432 22.76905528]\n",
            " [35.81392664 21.89868892]\n",
            " [28.61675702 40.60251418]\n",
            " [66.49829255 34.42891855]]\n",
            "\n",
            "Episode: 3\n",
            "[[36.04409393 37.33339441]\n",
            " [39.03426401 30.29520045]\n",
            " [40.552555   30.71169621]\n",
            " [38.91045245 40.87828532]\n",
            " [58.45650708 40.95455312]]\n",
            "\n",
            "Episode: 4\n",
            "[[41.23191739 45.63317994]\n",
            " [50.82485287 33.34175156]\n",
            " [55.245068   35.23123869]\n",
            " [60.61589845 42.64763848]\n",
            " [64.24268938 46.50580694]]\n",
            "\n",
            "Episode: 19996\n",
            "[[52.75025177 51.57409491]\n",
            " [56.77288556 54.32295368]\n",
            " [61.52475173 54.95199613]\n",
            " [70.98776743 55.5644305 ]\n",
            " [77.58067194 67.45046601]]\n",
            "\n",
            "Episode: 19997\n",
            "[[54.52064275 53.32850525]\n",
            " [57.60314737 54.79620068]\n",
            " [62.15263779 55.42556484]\n",
            " [68.15354995 55.13214648]\n",
            " [84.56211446 57.45799812]]\n",
            "\n",
            "Episode: 19998\n",
            "[[60.08604437 56.39747343]\n",
            " [63.64197317 57.83102065]\n",
            " [66.53833455 57.18192412]\n",
            " [71.58641908 55.99225565]\n",
            " [77.88160806 70.3644576 ]]\n",
            "\n",
            "Episode: 19999\n",
            "[[64.48476273 59.18270542]\n",
            " [70.09602618 60.03661385]\n",
            " [77.58691711 57.96626716]\n",
            " [81.76147207 59.492414  ]\n",
            " [97.3400852  69.26652716]]\n",
            "\n",
            "\n",
            "epsilon: 0.01\n",
            "Episode: 0\n",
            "[[28.9650196   0.        ]\n",
            " [33.1897002   0.        ]\n",
            " [37.6483753   2.50238047]\n",
            " [47.48020729  0.        ]\n",
            " [59.17019801  0.        ]]\n",
            "\n",
            "Episode: 1\n",
            "[[48.57130677  0.        ]\n",
            " [52.07960503  0.        ]\n",
            " [56.13501791  2.50238047]\n",
            " [61.98522964  4.40027473]\n",
            " [66.63230195  7.92189239]]\n",
            "\n",
            "Episode: 2\n",
            "[[53.5946721   0.        ]\n",
            " [57.8237611  10.21979154]\n",
            " [61.89998118  7.35946925]\n",
            " [65.56431216  8.91467689]\n",
            " [75.26049292 32.44488156]]\n",
            "\n",
            "Episode: 3\n",
            "[[58.61233251  5.47264173]\n",
            " [62.05133606 22.89643472]\n",
            " [67.06866195  7.35946925]\n",
            " [73.38219549  8.91467689]\n",
            " [73.75151749 42.17784338]]\n",
            "\n",
            "Episode: 4\n",
            "[[60.80709391 10.70288604]\n",
            " [63.75954522 22.89643472]\n",
            " [68.0069976  12.56864649]\n",
            " [76.23305451 18.36089197]\n",
            " [87.93304388 48.07316107]]\n",
            "\n",
            "Episode: 19996\n",
            "[[58.86114059 51.39314415]\n",
            " [62.35780677 54.15869408]\n",
            " [66.70680578 55.87307798]\n",
            " [68.98440402 56.8245033 ]\n",
            " [75.55394445 57.38247984]]\n",
            "\n",
            "Episode: 19997\n",
            "[[57.11059409 52.09132227]\n",
            " [61.84622399 54.15869408]\n",
            " [69.04972365 55.87307798]\n",
            " [74.4561002  56.59852911]\n",
            " [96.39022618 57.24510957]]\n",
            "\n",
            "Episode: 19998\n",
            "[[57.81671685 52.09132227]\n",
            " [61.54499732 54.76197838]\n",
            " [66.5543113  57.69256835]\n",
            " [70.48287321 56.59852911]\n",
            " [74.16620033 60.79472788]]\n",
            "\n",
            "Episode: 19999\n",
            "[[61.91026101 52.65346005]\n",
            " [64.9374137  56.09597278]\n",
            " [67.82535589 57.76621801]\n",
            " [68.85683172 56.59852911]\n",
            " [72.24450688 62.99801146]]\n",
            "\n",
            "\n",
            "epsilon: 0.001\n",
            "Episode: 0\n",
            "[[31.37929887  0.        ]\n",
            " [35.32794727  0.        ]\n",
            " [42.37033521  0.        ]\n",
            " [49.84845833  0.        ]\n",
            " [56.47954628  0.        ]]\n",
            "\n",
            "Episode: 1\n",
            "[[48.90842471  0.        ]\n",
            " [52.65124208  0.        ]\n",
            " [56.14761697  0.        ]\n",
            " [59.99027125  0.        ]\n",
            " [64.22218569  0.        ]]\n",
            "\n",
            "Episode: 2\n",
            "[[51.63782749  0.        ]\n",
            " [53.70473861  0.        ]\n",
            " [55.96847171  0.        ]\n",
            " [60.92579929  0.        ]\n",
            " [71.82251003  0.        ]]\n",
            "\n",
            "Episode: 3\n",
            "[[53.35007202  0.        ]\n",
            " [56.66454541  0.        ]\n",
            " [62.95026578  0.        ]\n",
            " [69.92993571  0.        ]\n",
            " [77.86206307  0.        ]]\n",
            "\n",
            "Episode: 4\n",
            "[[59.10557816  0.        ]\n",
            " [63.36319828  0.        ]\n",
            " [68.77622643  0.        ]\n",
            " [79.85024334  0.        ]\n",
            " [94.1340103   0.        ]]\n",
            "\n",
            "Episode: 19996\n",
            "[[53.09248687 51.60310773]\n",
            " [55.57558701 52.87275645]\n",
            " [58.42130367 55.09214703]\n",
            " [64.94154573 57.6305107 ]\n",
            " [76.79827259 63.45788134]]\n",
            "\n",
            "Episode: 19997\n",
            "[[56.43599152 51.60310773]\n",
            " [60.01544029 52.87275645]\n",
            " [64.27010564 55.09214703]\n",
            " [69.53699788 57.6305107 ]\n",
            " [86.85687974 63.45788134]]\n",
            "\n",
            "Episode: 19998\n",
            "[[58.54880122 51.60310773]\n",
            " [61.90290542 52.87275645]\n",
            " [67.10844777 55.09214703]\n",
            " [71.90305871 57.6305107 ]\n",
            " [85.2889868  63.45788134]]\n",
            "\n",
            "Episode: 19999\n",
            "[[56.74854677 51.60310773]\n",
            " [59.41770352 52.87275645]\n",
            " [62.5117154  55.09214703]\n",
            " [70.08475077 57.6305107 ]\n",
            " [91.68903307 62.88546264]]\n",
            "\n",
            "\n",
            "epsilon: 0.0001\n",
            "Episode: 0\n",
            "[[30.25253659  0.        ]\n",
            " [32.87321992  0.        ]\n",
            " [37.3089162   0.        ]\n",
            " [45.39505585  0.        ]\n",
            " [56.46706708  0.        ]]\n",
            "\n",
            "Episode: 1\n",
            "[[46.13788826  0.        ]\n",
            " [49.87840153  0.        ]\n",
            " [55.80826461  0.        ]\n",
            " [61.8194728   0.        ]\n",
            " [68.42591869  0.        ]]\n",
            "\n",
            "Episode: 2\n",
            "[[54.67522234  0.        ]\n",
            " [58.07184466  0.        ]\n",
            " [62.19333403  0.        ]\n",
            " [67.33673624  0.        ]\n",
            " [77.90629553  0.        ]]\n",
            "\n",
            "Episode: 3\n",
            "[[59.16301311  0.        ]\n",
            " [63.23440098  0.        ]\n",
            " [66.88997413  0.        ]\n",
            " [72.47121683  0.        ]\n",
            " [77.88299586  0.        ]]\n",
            "\n",
            "Episode: 4\n",
            "[[60.28629698  0.        ]\n",
            " [63.82592675  0.        ]\n",
            " [68.3781766   0.        ]\n",
            " [72.56027924  0.        ]\n",
            " [77.00591475  0.        ]]\n",
            "\n",
            "Episode: 19996\n",
            "[[53.07597725 50.0515991 ]\n",
            " [56.59489455 51.13284451]\n",
            " [61.24056658 52.36038267]\n",
            " [64.78012968 56.7064157 ]\n",
            " [69.41491609 60.40030942]]\n",
            "\n",
            "Episode: 19997\n",
            "[[55.13323261 50.0515991 ]\n",
            " [57.20721159 51.13284451]\n",
            " [60.10057531 52.36038267]\n",
            " [64.94742004 56.7064157 ]\n",
            " [71.24433572 60.40030942]]\n",
            "\n",
            "Episode: 19998\n",
            "[[50.85747209 50.0515991 ]\n",
            " [53.46017147 51.13284451]\n",
            " [56.82306329 52.36038267]\n",
            " [63.3067536  56.7064157 ]\n",
            " [71.01577106 60.40030942]]\n",
            "\n",
            "Episode: 19999\n",
            "[[57.68767817 50.00134111]\n",
            " [60.98259558 51.13284451]\n",
            " [64.04304655 52.36038267]\n",
            " [71.48360353 56.7064157 ]\n",
            " [74.17069524 60.40030942]]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set parameters \n",
        "num_episodes = 20000 \n",
        "gamma = 0.95 #given\n",
        "learning_rate = 0.1 #0.95 #0.85 alpha // small fixed \n",
        "epsilon = 10 ** -1 #go from 1e-1 to 1e-5\n",
        "\n",
        "while epsilon > 10 ** -5:\n",
        "#function call trying some decreasing sequences of  ùúñ  (1/t etc.) with small fixed learning rate  ùõº.\n",
        " #Q = Q_learn(num_episodes, gamma, learning_rate, x/10) #0.6 - .. to make it decreasing order\n",
        " print(\"epsilon:\", epsilon)\n",
        " Q_learn(num_episodes, gamma, learning_rate, epsilon)\n",
        " print()\n",
        " epsilon = epsilon/10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1xSOvuhFiY9"
      },
      "source": [
        "source: https://gym.openai.com/envs/NChain-v0/ \n",
        "\n",
        "\n",
        "source: http://ceit.aut.ac.ir/~shiry/lecture/machine-learning/papers/BRL-2000.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e99ZlQ8YCGpL"
      },
      "source": [
        "\n",
        "## Question 4 (2 points)\n",
        "\n",
        "Verify that the optimal $Q^*$ value obtained using Q-learning is same as the optimal value function $V^*$ (for the corresponding optimal action). You would have to first define the MDP corresponding to Chain enviroment.\n",
        "\n",
        "\n",
        "Hint: \n",
        "\n",
        "* \"Define the MDP..\" means to define \"$P_a(s,s')$\" and \"$R_a(s,s')$\" tables for the Chain environment. Its good to know that gym simulator that you use in Q3 takes the correct action with a probability of 0.8 (slip=0.2). This information is useful in defining $P_a(s,s')$ correctly (you can check the nchain.py file by going to the folder specified by 'print(gym.envs.toy_text)' command).\n",
        "\n",
        "* Once the \"p\" and \"r\" tables are defined, you can use your VI implementation from Q2 to compute the \"V*(s)\". Then the following relation must hold: $V^*(s) = max_a Q^*(s)$ for all s. This proves that the optimal solution can be obtained using Q-learning without knowing the underlying environment model (MDP), unlike the VI algorithm, which needs $P_a,R_a$ information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACKRGPbyAgpa",
        "outputId": "c0c7880a-0706-4234-8a99-a259811fa724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'gym.envs.toy_text' from '/usr/local/lib/python3.7/dist-packages/gym/envs/toy_text/__init__.py'>\n"
          ]
        }
      ],
      "source": [
        "#Define the MDP \n",
        "#the correct action with a probability of 0.8 (slip=0.2)\n",
        "#you can check the nchain.py file by going to the folder specified by 'print(gym.envs.toy_text)' command\n",
        "\n",
        "print(gym.envs.toy_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "#Defining the MDP\n",
        "states = [0, 1, 2, 3, 4] #State_space\n",
        "actions = [0,1] #0 = forward, 1 = backward \n",
        "P = 0.8 #the correct action with a probability of 0.8 (slip=0.2)\n",
        "rewards = [2, 0, 0, 0, 10] #Reward function \n",
        "V = [0, 0, 0, 0, 0] #Initial value-function\n",
        "\n",
        "\n",
        "#Calculate the value-function\n",
        "for _ in range(20000):\n",
        "    prevV = copy.deepcopy(V)\n",
        "    \n",
        "    s_backward = states[0] #Back to start\n",
        "    for state in states:\n",
        "        maxV = 0\n",
        "        if state <= 3:\n",
        "            s_forward = states[state+1] #Moving forward\n",
        "        else:\n",
        "            s_forward = states[4] #Standin still\n",
        "        \n",
        "        for action in actions:\n",
        "            if action == 0:\n",
        "                p = 0.8 #the correct action\n",
        "            else:\n",
        "                p = 0.2 #slip\n",
        "            \n",
        "            #calculates the current expected value for given state and action\n",
        "            currentV = p*(rewards[s_forward] + gamma*(prevV[s_forward])) + (1-p)*(rewards[s_backward] + gamma*(prevV[s_backward]))\n",
        "\n",
        "            if (currentV >= maxV):\n",
        "                curr_pi = action\n",
        "                maxV = currentV\n",
        "                    \n",
        "        #update value matrix with highest expected value at this state      \n",
        "        V[state] = maxV \n",
        "\n",
        "\n",
        "#print(\"Q:\", Q_learn(num_episodes, gamma, learning_rate, 0.01))\n",
        "\n",
        "def Q_learn_2(gamma, learning_rate, epsilon): \n",
        "\n",
        "# initialize the Q table 5 states and 2 actions \n",
        " Q = np.zeros([5, 2])\n",
        "\n",
        "#Training Q table, code taken from q_learning_intro.ipynb\n",
        " for episode in range(20000):\n",
        "\t  state = env.reset()\n",
        "\t  done = False\n",
        "\t  while done == False:\n",
        "        # First we select an action:\n",
        "\t\t  if random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
        "\t\t\t  action = env.action_space.sample() # Explore action space\n",
        "\t\t  else:\n",
        "\t\t\t  action = np.argmax(Q[state,:]) # Exploit learned values\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "\t\t  new_state, reward, done, info = env.step(action)\n",
        "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "\t\t  prediction_error = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
        "\t\t  Q[state,action] += learning_rate*prediction_error \n",
        "\t\t  state = new_state\n",
        " return Q\n",
        "\n",
        "Q = Q_learn_2(gamma, learning_rate, epsilon)\n",
        "print(\"Q:\", Q)\n",
        "print()\n",
        "print(\"V:\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSGS56JQf6n4",
        "outputId": "b8f31fc5-4982-4a82-9346-10890c34f18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: [[56.43698246 46.21150997]\n",
            " [60.24972408 45.62109877]\n",
            " [64.82972702 51.82878699]\n",
            " [69.64060595 41.53297344]\n",
            " [74.63997573 58.08063029]]\n",
            "\n",
            "V: [78.23615999999971, 82.8569599999997, 88.9369599999997, 96.9369599999997, 96.9369599999997]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the result, the optimal $Q^*$ value obtained using Q-learning is not same as the optimal value function $V^*$. As I can observe in the result of Question 3, the algorithm seem to be sensitive according to hyperparameters. To estimate the reason for obtaining different results from the theory, MDP or initial values are wrong. "
      ],
      "metadata": {
        "id": "qvKq5COF3BqY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffw6ucXoFdRW"
      },
      "source": [
        "## Question 5 (3 point)\n",
        "\n",
        "* what was the significance of exploration in RL ? How did you control it in the case Q-learning algorithm ? (1.5 pts)\n",
        "\n",
        "Exploration is significant in that reinforcement learning agents depend imperatively on exploration to obtain informative data for the learning process and it helps to resolve the lack of information issue which could impede effective learning.\n",
        "\n",
        "It is necessary that the algorithm explores its environment by taking actions and observing their consequences when the algorithm usually does not have any prior knowledge regarding the task. Moreover, in exploration, the agent improves its knowledge about each action that could lead to the highest reward in opposite to exploitation which chooses the greedy approach. \n",
        "\n",
        "\n",
        "In the Q-learning algorithm, Q-learning calculates the rewards for a given chance of exploring (epsilon) in an unknown environment. First of all, we select an action by flipping a skewed coin either explore action space or explore learned values. And then, we perform the action and receive feedback from the environment. And then, we learn from the experience by updating the Q-value of the selected action.\n",
        "\n",
        "\n",
        "Source: https://ai-ml-analytics.com/reinforcement-learning-exploration-vs-exploitation-tradeoff/\n",
        "\n",
        "https://analyticsindiamag.com/exploration-reinforcement-learning/#:~:text=A%20classical%20approach%20to%20any,their%20own%20tails%20to%20eternity.\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Multi-armed_bandit#:~:text=In%20probability%20theory%20and%20machine,when%20each%20choice's%20properties%20are\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU1_ddiQ-QRz"
      },
      "source": [
        "* Briefly discuss the k-armed bandit problem formulation and it's distinguishing feature as a special case of the reinforcement learning problem formulation. (1.5 pts)\n",
        "\n",
        "\n",
        "The multi-armed bandit problem (K-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing alternative choices in a way that maximizes their expected gain and each choice's properties are only partially known at the time of allocation. and it may become better understood by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation trade-off dilemma.\n",
        "\n",
        "source: https://en.wikipedia.org/wiki/Multi-armed_bandit#:~:text=In%20probability%20theory%20and%20machine,when%20each%20choice's%20properties%20are\n",
        "\n",
        "k-armed bandit problem formulation has distinguishing features as classic reinforcement learning. First, in the k-armed bandit problem, we have k different options to take, each option with a different underlying probability distribution to receive a reward that is unknown at the start. In opposite to supervised learning, we don't know which option is best to get maximized reward, so. we need to find out which option is best based on previous knowledge (exploiting) or at random to find new possibilities (exploring) by trial and error. The optimal decision will probably be somewhere in between the extremes (always explore or always exploit) and will depend on the number of decisions to make. For example, if we use the greedy method that does not use any exploration, which means we always take the same option for all iterations. Instead, if we choose to explore that test-out option randomly then we can have a better chance to find the best option by seeing if there is a better alternative. \n",
        "\n",
        "\n",
        "source: https://towardsdatascience.com/reinforcement-learning-multi-arm-bandit-implementation-5399ef67b24b\n",
        "\n",
        "https://blog.dominodatalab.com/k-armed-bandit-problem#:~:text=In%20its%20basic%20form%2C%20the,chances%20of%20making%20a%20profit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNNYjz6AFdRW"
      },
      "source": [
        "## Note\n",
        "\n",
        "* Until now, we have described algorithms for when no. of states and actions are finite. In coming weeks, you will be taught how to extend these methods to continous state enviroments like ATARI games.\n",
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment5-2022_Bitnoori_Lee.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
